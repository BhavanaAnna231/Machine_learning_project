{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, concatenate, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sentences' ids, which will be processed to become our input and target data.\n",
    "conv_lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = []\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating question inputs and answer targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])\n",
    "        \n",
    "# Compare lengths of questions and answers\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Original questions and answers=======\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "======Cleaned questions and answers===\n",
      "not the hacking and gagging and spitting part please\n",
      "okay then how about we try out some french cuisine saturday night\n"
     ]
    }
   ],
   "source": [
    "print(\"======Original questions and answers=======\")\n",
    "print(questions[2])\n",
    "print(answers[2])\n",
    "\n",
    "print(\"\\n======Cleaned questions and answers===\")\n",
    "print(clean_questions[2])\n",
    "print(clean_answers[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting questions and answers with appropriate length (<20 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    443232.000000\n",
      "mean         10.872094\n",
      "std          12.215895\n",
      "min           0.000000\n",
      "25%           4.000000\n",
      "50%           7.000000\n",
      "75%          14.000000\n",
      "max         555.000000\n",
      "Name: counts, dtype: float64\n",
      "16.0\n",
      "19.0\n",
      "24.0\n",
      "32.0\n",
      "58.0\n"
     ]
    }
   ],
   "source": [
    "# Find the length of sentences\n",
    "lengths = []\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
    "\n",
    "print(lengths['counts'].describe())\n",
    "\n",
    "print(np.percentile(lengths, 80))\n",
    "print(np.percentile(lengths, 85))\n",
    "print(np.percentile(lengths, 90))\n",
    "print(np.percentile(lengths, 95))\n",
    "print(np.percentile(lengths, 99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose length of sequences to be of maximum 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "VOCAB_SIZE = 20000  # Size of the vocabulary\n",
    "EMBEDDING_DIM = 100  # Dimension of word embeddings\n",
    "MAX_LEN = 20  # Max length of input sentences\n",
    "NUM_SAMPLES = 60000  # Number of training samples\n",
    "GLOVE_DIR = 'glove-global-vectors-for-word-representation'\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 256\n",
    "GLOVE_URL = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "GLOVE_FILE = \"glove.6B.zip\"\n",
    "\n",
    "if not os.path.exists(GLOVE_FILE):\n",
    "    os.system(f\"curl -O {GLOVE_URL}\")\n",
    "\n",
    "if not os.path.exists(GLOVE_DIR):\n",
    "    os.makedirs(GLOVE_DIR)\n",
    "    with zipfile.ZipFile(GLOVE_FILE, 'r') as zip_ref:\n",
    "        zip_ref.extractall(GLOVE_DIR)\n",
    "\n",
    "embedding_index = {}\n",
    "glove_path = os.path.join(GLOVE_DIR, f'glove.6B.{EMBEDDING_DIM}d.txt')\n",
    "\n",
    "with open(glove_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coef = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138335\n",
      "138335\n",
      "he is doing the best he can charles\n",
      "he is the son of the captain for chrissakes you would think he would be able to operate this thing\n",
      "\n",
      "i doubt very much that one even exists\n",
      "what are you talking about\n",
      "\n",
      "what did you do with rennie\n",
      "nothing i went to her cabin and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove questions and answers that are shorter than 1 word and longer than 20 words.\n",
    "min_line_length = 2\n",
    "max_line_length = 20\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "for i, question in enumerate(clean_questions):\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "for i, answer in enumerate(short_answers_temp):\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "        \n",
    "print(len(short_questions))\n",
    "print(len(short_answers))\n",
    "\n",
    "\n",
    "r = np.random.randint(1,len(short_questions))\n",
    "for i in range(r, r+3):\n",
    "    print(short_questions[i])\n",
    "    print(short_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_text = short_questions[:NUM_SAMPLES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put BOS tag and EOS tag for decoder input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS> not the hacking and gagging and spitting part please <EOS>',\n",
       " '<BOS> okay then how about we try out some french cuisine saturday night <EOS>',\n",
       " '<BOS> forget it <EOS>',\n",
       " '<BOS> let me see what i can do <EOS>',\n",
       " '<BOS> right see you are ready for the quiz <EOS>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tagger(input_text):\n",
    "  bos = \"<BOS> \"\n",
    "  eos = \" <EOS>\"\n",
    "  final_target = [bos + text + eos for text in input_text] \n",
    "  return final_target\n",
    "\n",
    "short_answers = short_answers[:NUM_SAMPLES]\n",
    "decoder_input_text = tagger(short_answers)\n",
    "decoder_input_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder_input_text))\n",
    "print(len(decoder_input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Vocabulary (VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2idx:  {'<bos>': 1, '<eos>': 2, 'you': 3, 'i': 4, 'the': 5}\n",
      "\n",
      "idx2word:  {1: '<bos>', 2: '<eos>', 3: 'you', 4: 'i', 5: 'the'}\n",
      "word2idx length 19999\n",
      "idx2word length 19999\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words= VOCAB_SIZE, filters='')\n",
    "\n",
    "def vocab_creater(text_lists, VOCAB_SIZE):  \n",
    "  tokenizer.fit_on_texts(text_lists)\n",
    "  dictionary = tokenizer.word_index\n",
    "  \n",
    "  word2idx = {}\n",
    "  idx2word = {}\n",
    "  for k, v in dictionary.items():\n",
    "      if v < VOCAB_SIZE:\n",
    "          word2idx[k] = v\n",
    "          idx2word[v] = k\n",
    "      if v >= VOCAB_SIZE-1:\n",
    "          continue\n",
    "          \n",
    "  return word2idx, idx2word\n",
    "\n",
    "word2idx, idx2word = vocab_creater(text_lists=encoder_input_text+decoder_input_text, VOCAB_SIZE=VOCAB_SIZE)\n",
    "\n",
    "#print first few key/value pairs\n",
    "word2idx_first5pairs = {k: word2idx[k] for k in list(word2idx)[:5]}\n",
    "print('word2idx: ', word2idx_first5pairs)\n",
    "\n",
    "idx2word_first5pairs = {k: idx2word[k] for k in list(idx2word)[:5]}\n",
    "print('\\nidx2word: ', idx2word_first5pairs)\n",
    "\n",
    "# Check the length of the dictionaries.\n",
    "print('word2idx length', len(word2idx))\n",
    "print('idx2word length', len(idx2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Bag of words to Bag of IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_sequences:\n",
      " [[51, 4, 136, 19, 35, 293, 36, 17814, 54, 12, 6, 101, 36, 3], [7, 5, 9560, 18, 7938, 18, 6088, 393, 139], [3, 13, 465, 16, 48, 12, 6, 45, 941, 12, 6, 28, 159, 164], [1551, 54, 124, 19, 77, 147, 3094, 10, 1488], [17815, 991, 17816, 25, 6, 29, 354]]\n",
      "\n",
      "decoder_sequences:\n",
      " [[1, 7, 5, 9560, 18, 7938, 18, 6088, 393, 139, 2], [1, 101, 86, 42, 37, 19, 221, 48, 83, 1101, 1996, 150, 2], [1, 288, 9, 2], [1, 122, 16, 69, 14, 4, 52, 11, 2], [1, 55, 69, 3, 13, 356, 26, 5, 10775, 2]]\n"
     ]
    }
   ],
   "source": [
    "def text2seq(tokenizer, encoder_text, decoder_text, VOCAB_SIZE):\n",
    "  encoder_sequences = tokenizer.texts_to_sequences(encoder_text)\n",
    "  decoder_sequences = tokenizer.texts_to_sequences(decoder_text)\n",
    "  \n",
    "  return encoder_sequences, decoder_sequences\n",
    "\n",
    "encoder_sequences, decoder_sequences = text2seq(tokenizer, encoder_input_text, decoder_input_text, VOCAB_SIZE)\n",
    "print('encoder_sequences:\\n', encoder_sequences[:5])\n",
    "print('\\ndecoder_sequences:\\n', decoder_sequences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding (MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_data:\n",
      " [[  51    4  136 ...    0    0    0]\n",
      " [   7    5 9560 ...    0    0    0]\n",
      " [   3   13  465 ...    0    0    0]\n",
      " ...\n",
      " [ 603 3243 2132 ...    0    0    0]\n",
      " [ 609   22    6 ...    0    0    0]\n",
      " [  40   22   33 ...    0    0    0]]\n",
      "\n",
      "decoder_input_data:\n",
      " [[  1   7   5 ...   0   0   0]\n",
      " [  1 101  86 ...   0   0   0]\n",
      " [  1 288   9 ...   0   0   0]\n",
      " ...\n",
      " [  1  68 113 ...   0   0   0]\n",
      " [  1  40  22 ...   0   0   0]\n",
      " [  1  40  22 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "def padding(encoder_sequences, decoder_sequences, MAX_LEN):\n",
    "  \n",
    "  encoder_input_data = pad_sequences(encoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "  decoder_input_data = pad_sequences(decoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "  \n",
    "  return encoder_input_data, decoder_input_data\n",
    "\n",
    "encoder_input_data, decoder_input_data = padding(encoder_sequences, decoder_sequences, MAX_LEN)\n",
    "print('encoder_input_data:\\n', encoder_input_data)\n",
    "print('\\ndecoder_input_data:\\n', decoder_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the Data to neural network shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(encoder_sequences)\n",
    "\n",
    "def decoder_output_creator(decoder_input_data, num_samples, MAX_LEN, VOCAB_SIZE):\n",
    "  decoder_output_data = np.zeros((num_samples, MAX_LEN, VOCAB_SIZE), dtype=\"float32\")\n",
    "  for i, seqs in enumerate(decoder_input_data):\n",
    "      for t, seq in enumerate(seqs):\n",
    "          if t > 0:                \n",
    "                decoder_output_data[i][t][seq] = 1.  #decoder_output_data[i, t, seq] = 1.\n",
    "  \n",
    "  return decoder_output_data\n",
    "\n",
    "decoder_output_data = decoder_output_creator(decoder_input_data, num_samples, MAX_LEN, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 20)\n",
      "(60000, 20)\n",
      "(60000, 20, 20000)\n"
     ]
    }
   ],
   "source": [
    "print (encoder_input_data.shape)\n",
    "print (decoder_input_data.shape)\n",
    "print (decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding (EMBEDDING_DIM)\n",
    "\n",
    "We use Pretraind Word2Vec Model from Glove. We can create embedding layer with Glove with 3 steps:\n",
    "1. Call Glove file\n",
    "1. Create Embedding Matrix from our Vocabulary\n",
    "1. Create Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Call Glove file\n",
    "def glove_100d_dictionary(glove_dir):\n",
    "  embeddings_index = {}\n",
    "  f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "  for line in f:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      coefs = np.asarray(values[1:], dtype='float32')\n",
    "      embeddings_index[word] = coefs\n",
    "  f.close()\n",
    "  return embeddings_index\n",
    "\n",
    "embeddings_index = glove_100d_dictionary(GLOVE_DIR)\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100)\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "#Create Embedding Matrix from our Vocabulary\n",
    "def embedding_matrix_creater(max_words, embedding_dimension):\n",
    "  embedding_matrix = np.zeros((max_words, embedding_dimension))  #np.zeros((len(word2idx) + 1, embedding_dimension))\n",
    "  for word, i in word2idx.items():\n",
    "        if i < max_words:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "  return embedding_matrix\n",
    "\n",
    "embedding_matrix = embedding_matrix_creater(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "print(embedding_matrix.shape)\n",
    "print((len(word2idx) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Embedding Layer\n",
    "def embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix):\n",
    "  \n",
    "  embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                              output_dim = EMBEDDING_DIM,\n",
    "                              input_length = MAX_LEN,\n",
    "                              weights = [embedding_matrix],\n",
    "                              trainable = False)\n",
    "  return embedding_layer\n",
    "\n",
    "embedding_layer = embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix)\n",
    "\n",
    "embedding_layer2 = embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, None, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split data for training validation & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_spliter(encoder_input_data, decoder_input_data, test_size1=0.2, test_size2=0.3):  \n",
    "  en_train, en_test, de_train, de_test = train_test_split(encoder_input_data, decoder_input_data, test_size=test_size1)\n",
    "  en_train, en_val, de_train, de_val = train_test_split(en_train, de_train, test_size=test_size2)\n",
    "  \n",
    "  return en_train, en_val, en_test, de_train, de_val, de_test\n",
    "\n",
    "en_train, en_val, en_test, de_train, de_val, de_test = data_spliter(encoder_input_data, decoder_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Seq2Seq Neural Network Architecture\n",
    "\n",
    "Seq2Seq is a type of Encoder-Decoder model using RNN. It can be used as a model for machine interaction and machine translation. By learning a large number of sequence pairs, this model generates one from the other. More kindly explained, the definition of Seq2Seq is below:\n",
    "* Input: Text Data\n",
    "* Output: Text Data as well\n",
    "\n",
    "Below is our Seq2Seq Neural Network Architecture using LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    2000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 300), (None, 481200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 300),  481200      embedding_2[1][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 20000)  6020000     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,982,400\n",
      "Trainable params: 6,982,400\n",
      "Non-trainable params: 2,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_seq2seq_model(HIDDEN_DIM=300):\n",
    "    #set up the encoder\n",
    "    encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "    encoder_embedding = embedding_layer(encoder_inputs)\n",
    "    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "    decoder_embedding = embedding_layer(decoder_inputs)\n",
    "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
    "    outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
    "    \n",
    "    #create seq2seq model\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc']) #sparse_categorical_crossentropy as labels in a single integer array\n",
    "    \n",
    "    #create encoder model\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    #Create sampling/decoder model\n",
    "    decoder_state_input_h  = Input(shape=(HIDDEN_DIM,))\n",
    "    decoder_state_input_c = Input(shape=(HIDDEN_DIM,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_embedding2= embedding_layer(decoder_inputs)\n",
    "    decoder_outputs2, state_h2, state_c2 = decoder_LSTM(decoder_embedding2, initial_state=decoder_states_inputs)\n",
    "    decoder_states2 = [state_h2, state_c2]\n",
    "    decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n",
    "    \n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "def build_seq2seq_model2(HIDDEN_DIM=300):\n",
    "    #set up the encoder\n",
    "    encoder_inputs = Input(shape=(None, ), dtype='int32',)\n",
    "    encoder_embedding = embedding_layer2(encoder_inputs)\n",
    "    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, ), dtype='int32',)\n",
    "    decoder_embedding = embedding_layer2(decoder_inputs)\n",
    "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
    "    outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
    "    \n",
    "    #create seq2seq model\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc']) #sparse_categorical_crossentropy as labels in a single integer array\n",
    "    \n",
    "    #create encoder model\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    #Create sampling/decoder model\n",
    "    decoder_state_input_h  = Input(shape=(HIDDEN_DIM,))\n",
    "    decoder_state_input_c = Input(shape=(HIDDEN_DIM,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_embedding2= embedding_layer2(decoder_inputs)\n",
    "    decoder_outputs2, state_h2, state_c2 = decoder_LSTM(decoder_embedding2, initial_state=decoder_states_inputs)\n",
    "    decoder_states2 = [state_h2, state_c2]\n",
    "    decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n",
    "    \n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "model, encoder_model, decoder_model = build_seq2seq_model2(HIDDEN_DIM=300)\n",
    "model.summary()\n",
    "\n",
    "# model, encoder_model, decoder_model = build_seq2seq_model(HIDDEN_DIM=300)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 300), (None, 300) 481200    \n",
      "=================================================================\n",
      "Total params: 2,481,200\n",
      "Trainable params: 481,200\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    2000000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 300),  481200      embedding_2[2][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 20000)  6020000     lstm_2[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,501,200\n",
      "Trainable params: 6,501,200\n",
      "Non-trainable params: 2,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57000 samples, validate on 3000 samples\n",
      "Epoch 1/40\n",
      "57000/57000 [==============================] - 180s 3ms/step - loss: 2.1462 - acc: 0.6566 - val_loss: 1.0507 - val_acc: 0.8236\n",
      "Epoch 2/40\n",
      "57000/57000 [==============================] - 150s 3ms/step - loss: 0.6717 - acc: 0.8718 - val_loss: 0.4757 - val_acc: 0.8990\n",
      "Epoch 3/40\n",
      "57000/57000 [==============================] - 150s 3ms/step - loss: 0.3677 - acc: 0.9090 - val_loss: 0.3614 - val_acc: 0.9135\n",
      "Epoch 4/40\n",
      "57000/57000 [==============================] - 150s 3ms/step - loss: 0.2687 - acc: 0.9201 - val_loss: 0.2997 - val_acc: 0.9194\n",
      "Epoch 5/40\n",
      "57000/57000 [==============================] - 150s 3ms/step - loss: 0.2170 - acc: 0.9258 - val_loss: 0.2687 - val_acc: 0.9226\n",
      "Epoch 6/40\n",
      "57000/57000 [==============================] - 150s 3ms/step - loss: 0.1835 - acc: 0.9293 - val_loss: 0.2483 - val_acc: 0.9247\n",
      "Epoch 7/40\n",
      "57000/57000 [==============================] - 150s 3ms/step - loss: 0.1581 - acc: 0.9317 - val_loss: 0.2307 - val_acc: 0.9264\n",
      "Epoch 8/40\n",
      "57000/57000 [==============================] - 150s 3ms/step - loss: 0.1384 - acc: 0.9337 - val_loss: 0.2208 - val_acc: 0.9273\n",
      "Epoch 9/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.1238 - acc: 0.9352 - val_loss: 0.2099 - val_acc: 0.9284\n",
      "Epoch 10/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.1116 - acc: 0.9365 - val_loss: 0.2013 - val_acc: 0.9295\n",
      "Epoch 11/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.1008 - acc: 0.9376 - val_loss: 0.1957 - val_acc: 0.9299\n",
      "Epoch 12/40\n",
      "57000/57000 [==============================] - 150s 3ms/step - loss: 0.0911 - acc: 0.9386 - val_loss: 0.1887 - val_acc: 0.9306\n",
      "Epoch 13/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.0832 - acc: 0.9393 - val_loss: 0.1819 - val_acc: 0.9314\n",
      "Epoch 14/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.0759 - acc: 0.9399 - val_loss: 0.1794 - val_acc: 0.9315\n",
      "Epoch 15/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.0699 - acc: 0.9404 - val_loss: 0.1783 - val_acc: 0.9319\n",
      "Epoch 16/40\n",
      "57000/57000 [==============================] - 150s 3ms/step - loss: 0.0645 - acc: 0.9410 - val_loss: 0.1719 - val_acc: 0.9321\n",
      "Epoch 17/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.0596 - acc: 0.9415 - val_loss: 0.1681 - val_acc: 0.9323\n",
      "Epoch 18/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.0552 - acc: 0.9422 - val_loss: 0.1670 - val_acc: 0.9329\n",
      "Epoch 19/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.0510 - acc: 0.9429 - val_loss: 0.1660 - val_acc: 0.9334\n",
      "Epoch 20/40\n",
      "57000/57000 [==============================] - 150s 3ms/step - loss: 0.0473 - acc: 0.9434 - val_loss: 0.1630 - val_acc: 0.9336\n",
      "Epoch 21/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.0439 - acc: 0.9440 - val_loss: 0.1650 - val_acc: 0.9334\n",
      "Epoch 22/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.0407 - acc: 0.9445 - val_loss: 0.1609 - val_acc: 0.9341\n",
      "Epoch 23/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.0378 - acc: 0.9448 - val_loss: 0.1604 - val_acc: 0.9341\n",
      "Epoch 24/40\n",
      "57000/57000 [==============================] - 151s 3ms/step - loss: 0.0352 - acc: 0.9451 - val_loss: 0.1601 - val_acc: 0.9340\n",
      "Epoch 25/40\n",
      "57000/57000 [==============================] - 152s 3ms/step - loss: 0.0328 - acc: 0.9454 - val_loss: 0.1591 - val_acc: 0.9344\n",
      "Epoch 26/40\n",
      "57000/57000 [==============================] - 153s 3ms/step - loss: 0.0307 - acc: 0.9456 - val_loss: 0.1574 - val_acc: 0.9346\n",
      "Epoch 27/40\n",
      "57000/57000 [==============================] - 152s 3ms/step - loss: 0.0287 - acc: 0.9458 - val_loss: 0.1585 - val_acc: 0.9346\n",
      "Epoch 28/40\n",
      "57000/57000 [==============================] - 154s 3ms/step - loss: 0.0272 - acc: 0.9459 - val_loss: 0.1575 - val_acc: 0.9347\n",
      "Epoch 29/40\n",
      "57000/57000 [==============================] - 154s 3ms/step - loss: 0.0256 - acc: 0.9460 - val_loss: 0.1582 - val_acc: 0.9349\n",
      "Epoch 30/40\n",
      "57000/57000 [==============================] - 154s 3ms/step - loss: 0.0242 - acc: 0.9462 - val_loss: 0.1557 - val_acc: 0.9351\n",
      "Epoch 31/40\n",
      "57000/57000 [==============================] - 154s 3ms/step - loss: 0.0229 - acc: 0.9463 - val_loss: 0.1577 - val_acc: 0.9352\n",
      "Epoch 32/40\n",
      "57000/57000 [==============================] - 154s 3ms/step - loss: 0.0218 - acc: 0.9465 - val_loss: 0.1562 - val_acc: 0.9352\n",
      "Epoch 33/40\n",
      "57000/57000 [==============================] - 154s 3ms/step - loss: 0.0208 - acc: 0.9466 - val_loss: 0.1569 - val_acc: 0.9348\n",
      "Epoch 34/40\n",
      "57000/57000 [==============================] - 154s 3ms/step - loss: 0.0198 - acc: 0.9468 - val_loss: 0.1582 - val_acc: 0.9354\n",
      "Epoch 35/40\n",
      "57000/57000 [==============================] - 155s 3ms/step - loss: 0.0189 - acc: 0.9469 - val_loss: 0.1570 - val_acc: 0.9355\n",
      "Epoch 36/40\n",
      "57000/57000 [==============================] - 155s 3ms/step - loss: 0.0180 - acc: 0.9471 - val_loss: 0.1585 - val_acc: 0.9352\n",
      "Epoch 37/40\n",
      "57000/57000 [==============================] - 155s 3ms/step - loss: 0.0171 - acc: 0.9474 - val_loss: 0.1577 - val_acc: 0.9356\n",
      "Epoch 38/40\n",
      "57000/57000 [==============================] - 155s 3ms/step - loss: 0.0164 - acc: 0.9474 - val_loss: 0.1599 - val_acc: 0.9356\n",
      "Epoch 39/40\n",
      "57000/57000 [==============================] - 155s 3ms/step - loss: 0.0156 - acc: 0.9476 - val_loss: 0.1598 - val_acc: 0.9355\n",
      "Epoch 40/40\n",
      "57000/57000 [==============================] - 155s 3ms/step - loss: 0.0149 - acc: 0.9478 - val_loss: 0.1585 - val_acc: 0.9358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f99ec1b28d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcp = ModelCheckpoint('best_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_output_data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05,\n",
    "          callbacks= [mcp]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: i am fine you rest\n",
      "Response: there's there's there's\n",
      "-\n",
      "Input: i will wait here if you do not mind\n",
      "Response: there's there's\n",
      "-\n",
      "Input: oh huh\n",
      "Response: let's let's let's\n",
      "-\n",
      "Input: robert i\n",
      "Response: there's uhhuh\n",
      "-\n",
      "Input: you are on the grift same as me\n",
      "Response: there's there's\n",
      "-\n",
      "Input: how are you doing\n",
      "Response: who's who's\n",
      "-\n",
      "Input: i am going to tell him you said that\n",
      "Response: there's there's there's\n",
      "-\n",
      "Input: glass will cut glass mrs langtry do you know where it was purchased\n",
      "Response: there's let's there's\n",
      "-\n",
      "Input: umm three i believe but i am not sure\n",
      "Response: there's there's\n",
      "-\n",
      "Input: i ai not going to jail and i ai not doing that probation thing again\n",
      "Response: there's there's there's\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = word2idx['<bos>']\n",
    "    eos = word2idx['<eos>']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(MAX_LEN):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)        \n",
    "        # Sample a token\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = idx2word[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]  # Update states\n",
    "\n",
    "    return ' '.join(output_sentence)\n",
    "\n",
    "for index in range(10):\n",
    "    i = np.random.randint(1, len(encoder_input_data))\n",
    "    input_seq = encoder_input_data[i:i+1]\n",
    "    translation = translate_sentence(input_seq)\n",
    "    print('-')\n",
    "    print('Input:', short_answers[i])\n",
    "    print('Response:', translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
